import requests
import csv
import time
import os
import sys
import concurrent.futures
import re
import unicodedata

# Configuration - Update this to match your export script configuration
ZENDESK_SUBDOMAIN = "z3n-xxx" # Zendesk subdomain Ex. zen-mydomain
ZENDESK_EMAIL = "xxxx@mycompany.com" # Your login email at Zendesk plataform
ZENDESK_API_TOKEN = "xxxxxyyyyyzzzzzwwwww" # Your Zendesk API Token

# Input File - The CSV generated by the previous script
INPUT_CSV_FILE = '/tmp/ticket_fields.csv'

# Locale Mapping for AGENT Titles (Standard)
AGENT_LOCALE_MAP = {
    'title_pt': 'pt-BR',    # Default
    'title_en': 'en-US',
    'title_es': 'es'
}

# Locale Mapping for PORTAL Titles (End User)
PORTAL_LOCALE_MAP = {
    'title_in_portal_pt': 'pt-BR',
    'title_in_portal_en': 'en-US',
    'title_in_portal_es': 'es'
}

DEFAULT_LOCALE = 'pt-BR'

# API Endpoints
BASE_URL = f'https://{ZENDESK_SUBDOMAIN}.zendesk.com/api/v2/dynamic_content/items.json'
LOCALES_URL = f'https://{ZENDESK_SUBDOMAIN}.zendesk.com/api/v2/locales.json'
VARIANTS_URL_TEMPLATE = f'https://{ZENDESK_SUBDOMAIN}.zendesk.com/api/v2/dynamic_content/items/{{id}}/variants.json'
VARIANT_INSTANCE_URL_TEMPLATE = f'https://{ZENDESK_SUBDOMAIN}.zendesk.com/api/v2/dynamic_content/items/{{item_id}}/variants/{{variant_id}}.json'

# Debug Mode
DEBUG = True

def get_headers():
    return {
        'Content-Type': 'application/json',
    }

class ZendeskDCImporter:
    def __init__(self):
        self.session = requests.Session()
        self.session.auth = (f'{ZENDESK_EMAIL}/token', ZENDESK_API_TOKEN)
        self.session.headers.update(get_headers())
        self.existing_items = {} 
        self.locale_id_map = {}  # Map ID (int) -> Locale String
        self.locale_str_map = {} # Map Locale String -> ID

    def get_user_confirmation(self, prompt):
        """
        Helper to get yes/no input from user.
        """
        while True:
            response = input(f"{prompt} (y/n): ").strip().lower()
            if response in ['y', 'yes']: return True
            if response in ['n', 'no']: return False
            print("Please enter 'y' or 'n'.")

    def _sanitize_name(self, text):
        """
        Sanitizes text to meet Zendesk DC Name strict requirements:
        Only A-Z, 0-9, _, and : are allowed.
        """
        if not text:
            return ""
        text = unicodedata.normalize('NFKD', str(text)).encode('ASCII', 'ignore').decode('utf-8')
        # Allow A-Z, 0-9, _, and : (for categories)
        text = re.sub(r'[^A-Za-z0-9_:]+', '_', text)
        text = text.strip('_')
        return text

    def _normalize_content(self, text):
        """
        Collapses multiple spaces into one and strips whitespace.
        """
        if not text:
            return ""
        return ' '.join(text.split())

    def _aggressive_normalize(self, text):
        """
        Aggressive Normalization: Removes ALL whitespace.
        'A - B' -> 'a-b'
        """
        if not text: return ""
        return re.sub(r'\s+', '', str(text)).lower()

    def _print_progress(self, current, total, start_time, prefix="Progress", suffix=""):
        percent = current / total if total > 0 else 0
        bar_length = 30
        filled_length = int(bar_length * percent)
        bar = '█' * filled_length + '-' * (bar_length - filled_length)
        
        # Calculate ETA
        elapsed_time = time.time() - start_time
        if current > 0:
            estimated_total = elapsed_time / percent
            remaining = estimated_total - elapsed_time
            eta_str = time.strftime("%H:%M:%S", time.gmtime(remaining))
        else:
            eta_str = "--:--:--"

        print(f'\r{prefix}: |{bar}| {int(percent * 100)}% ({current}/{total}) [ETA: {eta_str}] {suffix}', end='', flush=True)

    def fetch_locales(self):
        print("Fetching Zendesk Locales...")
        response = self.session.get(LOCALES_URL)
        if response.status_code == 200:
            locales = response.json().get('locales', [])
            print(f"Found {len(locales)} locales in Zendesk:")
            
            for loc in locales:
                l_id = loc['id']
                l_code = loc['locale']
                self.locale_id_map[l_id] = l_code
                self.locale_str_map[l_code] = l_id
                self.locale_str_map[l_code.lower()] = l_id
                print(f" - {l_code}: ID {l_id}")
        else:
            print(f"Error fetching locales: {response.text}")
            sys.exit(1)

    def fetch_existing_items(self):
        print("Fetching existing Dynamic Content items from Zendesk...")
        url = BASE_URL
        count = 0
        total_items = 0
        
        while url:
            response = self.session.get(url)
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 10))
                print(f"Rate limited. Waiting {retry_after}s...")
                time.sleep(retry_after)
                continue
            
            if response.status_code != 200:
                print(f"Error fetching items: {response.text}")
                break
                
            data = response.json()
            items = data.get('items', [])
            total_items = data.get('count', total_items)
            
            if not items:
                break

            for item in items:
                name = item['name'].strip()
                item_id = item['id']
                
                variants_data = {}
                for var in item.get('variants', []):
                    loc_id = var.get('locale_id')
                    variants_data[loc_id] = {
                        'id': var['id'],
                        'content': var['content'],
                        'default': var['default']
                    }

                self.existing_items[name] = {
                    'id': item_id, 
                    'variants': variants_data,
                    'full_data': item
                }
                count += 1
                
                if total_items > 0:
                    # Using simplified progress for fetch since start time isn't critical here
                    percent = count / total_items
                    bar_length = 30
                    filled = int(bar_length * percent)
                    bar = '█' * filled + '-' * (bar_length - filled)
                    print(f"\rFetching: |{bar}| {int(percent * 100)}% ({count}/{total_items})", end='', flush=True)

            url = data.get('next_page')
            
        print(f"\nFinished loading {len(self.existing_items)} existing items.")

    def analyze_changes(self, rows, do_agent, do_portal, aggressive_norm):
        print("\nAnalyzing changes against current Zendesk data...")
        plan = {
            'create': [],
            'update': [],
            'skip': []
        }
        
        total_rows = len(rows)
        start_time = time.time()

        for i, row in enumerate(rows, 1):
            # Use the sanitized_name generated in process()
            dc_name = row['sanitized_name']
            self._print_progress(i, total_rows, start_time, prefix="Analyzing")
            
            raw_base_name = row['dc_name'] # The base name from CSV
            
            # Retrieve the Category pre-calculated in process()
            agent_category = row.get('agent_category')
            
            # ==============================================================================
            # 1. Process AGENT Title (Category::Name)
            # ==============================================================================
            if do_agent:
                agent_name_sanitized = f"{agent_category}::{self._sanitize_name(raw_base_name)}"
                
                self._analyze_single_item(
                    plan, 
                    name_key=agent_name_sanitized, 
                    row=row, 
                    map_dict=AGENT_LOCALE_MAP,
                    desc_suffix="(Agent Title)",
                    aggressive_norm=aggressive_norm
                )

            # ==============================================================================
            # 2. Process PORTAL Title (Portal_Category::Name)
            # ==============================================================================
            if do_portal:
                # Only process if there is actually portal content
                if row.get('title_in_portal_pt'):
                    if agent_category == "Tagger_Options":
                        # Special Case: Portal Options
                        portal_category = "Portal_Tagger_Options"
                    else:
                        # Standard Fields
                        portal_category = f"Portal_{agent_category}"
                        
                    portal_name_sanitized = f"{portal_category}::{self._sanitize_name(raw_base_name)}"
                    
                    self._analyze_single_item(
                        plan, 
                        name_key=portal_name_sanitized, 
                        row=row, 
                        map_dict=PORTAL_LOCALE_MAP,
                        desc_suffix="(Portal Title)",
                        aggressive_norm=aggressive_norm
                    )
        
        print("\nAnalysis Complete.")
        return plan

    def _analyze_single_item(self, plan, name_key, row, map_dict, desc_suffix, aggressive_norm):
        """
        Helper to analyze a single item (either Agent or Portal version)
        """
        # Intelligent Matching (Suffix search)
        matched_existing_item = None
        if name_key in self.existing_items:
            matched_existing_item = self.existing_items[name_key]
        else:
            for x in range(1, 21):
                suffixed_name = f"{name_key}_{x}"
                if suffixed_name in self.existing_items:
                    matched_existing_item = self.existing_items[suffixed_name]
                    break
        
        # Prepare Payload Helper Data
        item_data = {
            'name': name_key,
            'description': f"{row.get('dc_description', '')} {desc_suffix}",
            'row_data': row,
            'map_dict': map_dict
        }

        if not matched_existing_item:
            # CREATE
            plan['create'].append(item_data)
        else:
            # UPDATE CHECK
            existing = matched_existing_item
            updates_needed = []
            
            for col, target_locale_str in map_dict.items():
                new_content = row.get(col, '') # Already normalized (Standard) in process()
                if not new_content: continue

                target_locale_id = self._resolve_locale_id(target_locale_str)
                if target_locale_id is None: continue

                existing_var = existing['variants'].get(target_locale_id)
                
                if not existing_var:
                    updates_needed.append({
                        'type': 'add_variant',
                        'locale': target_locale_str,
                        'locale_id': target_locale_id,
                        'content': new_content
                    })
                else:
                    # COMPARE CONTENT
                    zen_content = existing_var['content'] or ""
                    
                    is_diff = False
                    if aggressive_norm:
                        # Aggressive: Strip ALL whitespace
                        if self._aggressive_normalize(zen_content) != self._aggressive_normalize(new_content):
                            is_diff = True
                    else:
                        # Standard: Collapsed whitespace (row is already collapsed, so just collapse zen)
                        if self._normalize_content(zen_content) != new_content:
                            is_diff = True

                    if is_diff:
                        if DEBUG:
                            print(f"\n[DEBUG] Diff for '{existing['full_data']['name']}' ({target_locale_str}):")
                            print(f"  CSV: {repr(new_content)}")
                            print(f"  ZEN: {repr(zen_content)}")

                        updates_needed.append({
                            'type': 'update_content',
                            'locale': target_locale_str,
                            'old': existing_var['content'],
                            'new': new_content,
                            'var_id': existing_var['id']
                        })
            
            if updates_needed:
                item_data['id'] = existing['id']
                item_data['changes'] = updates_needed
                plan['update'].append(item_data)
            else:
                plan['skip'].append(item_data)

    def count_breakdown(self, item_list):
        fields = 0
        options = 0
        for item in item_list:
            # The row 'type' tells us if it's an Option or a Field
            raw_type = item['row_data'].get('type', '')
            if raw_type == 'option':
                options += 1
            else:
                fields += 1
        return fields, options

    def print_preview(self, plan, rows):
        # Calculate Categories
        categories = set()
        
        # Check creates
        for item in plan['create']:
            name = item['name']
            if '::' in name:
                categories.add(name.split('::')[0])
        
        # Check updates
        for item in plan['update']:
             name = item['name']
             if '::' in name:
                categories.add(name.split('::')[0])

        # Calculate Field vs Option breakdowns
        c_fields, c_opts = self.count_breakdown(plan['create'])
        u_fields, u_opts = self.count_breakdown(plan['update'])
        s_fields, s_opts = self.count_breakdown(plan['skip'])

        print("\n" + "="*60)
        print(" PREVIEW REPORT")
        print("="*60)
        
        print(f"Items to CREATE: {len(plan['create']):<5} (Fields: {c_fields}, Options: {c_opts})")
        print(f"Items to UPDATE: {len(plan['update']):<5} (Fields: {u_fields}, Options: {u_opts})")
        print(f"Items to SKIP:   {len(plan['skip']):<5} (Fields: {s_fields}, Options: {s_opts})")
        print("-" * 60)
        print(f"Unique Categories Affected: {len(categories)}")
        print("="*60 + "\n")

    def execute_plan(self, plan):
        print("\nExecuting Changes...")
        MAX_WORKERS = 5
        
        # 1. Creates
        total_create = len(plan['create'])
        if total_create > 0:
            print(f"Starting {total_create} Creates with {MAX_WORKERS} threads...")
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                # Pass the whole item_data dictionary to create_item_api
                future_to_item = {executor.submit(self.create_item_api, item): item for item in plan['create']}
                
                completed = 0
                for future in concurrent.futures.as_completed(future_to_item):
                    completed += 1
                    self._print_progress(completed, total_create, start_time, prefix="Creating")
                    try:
                        future.result()
                    except Exception as e:
                        item = future_to_item[future]
                        print(f"\n[ERROR] Failed to create '{item['name']}': {e}")
            print()

        # 2. Updates
        total_update = len(plan['update'])
        if total_update > 0:
            print(f"Starting {total_update} Updates with {MAX_WORKERS} threads...")
            start_time = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_item = {executor.submit(self.update_item_api, item['id'], item['changes']): item for item in plan['update']}
                
                completed = 0
                for future in concurrent.futures.as_completed(future_to_item):
                    completed += 1
                    self._print_progress(completed, total_update, start_time, prefix="Updating")
                    try:
                        future.result()
                    except Exception as e:
                        item = future_to_item[future]
                        print(f"\n[ERROR] Failed to update '{item['name']}': {e}")
            print()
            
        print("\nFinished all operations.")

    def create_item_api(self, item_data):
        base_name = item_data['name']
        description = item_data['description']
        row = item_data['row_data']
        map_dict = item_data['map_dict']
        
        default_locale_id = self._resolve_locale_id(DEFAULT_LOCALE)
        if default_locale_id is None:
             raise ValueError(f"Could not find ID for default locale: {DEFAULT_LOCALE}")

        variants = []
        
        # 1. Find default content
        default_col = [k for k,v in map_dict.items() if v == DEFAULT_LOCALE]
        if default_col:
            default_content = row.get(default_col[0])
            if default_content:
                variants.append({
                    'locale_id': default_locale_id,
                    'default': True,
                    'content': default_content
                })
        
        # 2. Find other content
        for col, locale_str in map_dict.items():
            if locale_str == DEFAULT_LOCALE: continue
            content = row.get(col)
            if content:
                loc_id = self._resolve_locale_id(locale_str)
                if loc_id:
                    variants.append({
                        'locale_id': loc_id,
                        'default': False,
                        'content': content
                    })

        if not variants:
            return

        # Retry loop for collisions
        max_attempts = 20
        for attempt in range(max_attempts):
            if attempt == 0:
                name = base_name
            else:
                name = f"{base_name}_{attempt}"

            payload = {
                "item": {
                    "name": name,
                    "description": description,
                    "default_locale_id": default_locale_id,
                    "variants": variants
                }
            }
            
            response = self.session.post(BASE_URL, json=payload)
            
            # RATE LIMIT HANDLER (429)
            while response.status_code == 429:
                retry = int(response.headers.get('Retry-After', 5))
                time.sleep(retry)
                response = self.session.post(BASE_URL, json=payload)

            if response.status_code == 201:
                return 

            if response.status_code == 422:
                try:
                    res_json = response.json()
                    err_text = response.text.lower()
                    details = res_json.get('details', {})
                    is_collision = False
                    # Check for various collision indicators
                    if any(k in details for k in ['Placeholder', 'placeholder', 'Name', 'name']): is_collision = True
                    if any(msg in err_text for msg in ['in use', 'taken', 'em uso', 'já existe']): is_collision = True
                    
                    if is_collision:
                        continue
                except:
                    pass 

            raise Exception(f"Status {response.status_code} for name '{name}': {response.text}")

    def update_item_api(self, item_id, changes):
        for change in changes:
            if change['type'] == 'update_content':
                url = VARIANT_INSTANCE_URL_TEMPLATE.format(item_id=item_id, variant_id=change['var_id'])
                response = self.session.put(url, json={"variant": {"content": change['new']}})
            
            elif change['type'] == 'add_variant':
                url = VARIANTS_URL_TEMPLATE.format(id=item_id)
                loc_id = change['locale_id'] # Use pre-resolved ID
                
                payload = {
                    "variant": {
                        "locale_id": loc_id,
                        "default": (change['locale'] == DEFAULT_LOCALE),
                        "content": change['content']
                    }
                }
                response = self.session.post(url, json=payload)
            
            # RATE LIMIT HANDLER (429)
            while response.status_code == 429:
                retry = int(response.headers.get('Retry-After', 5))
                time.sleep(retry)
                if change['type'] == 'update_content':
                    response = self.session.put(url, json={"variant": {"content": change['new']}})
                else:
                    response = self.session.post(url, json=payload)

            if response.status_code not in [200, 201]:
                raise Exception(f"Status {response.status_code}: {response.text}")

    def _resolve_locale_id(self, locale_str):
        if not locale_str: return None
        return self.locale_str_map.get(locale_str) or self.locale_str_map.get(locale_str.lower())

    def process(self, csv_path):
        if not os.path.exists(csv_path):
            print(f"Error: File {csv_path} not found.")
            return

        # Ask User Preferences First
        print("\n--- Import Options ---")
        do_agent = self.get_user_confirmation("Import/Update AGENT Titles (Ticket Field Names)?")
        do_portal = self.get_user_confirmation("Import/Update PORTAL Titles (End-user View)?")
        aggressive_norm = self.get_user_confirmation("Enable AGGRESSIVE normalization (ignore all whitespace/newlines differences)?")
        print("----------------------\n")

        self.fetch_locales()
        self.fetch_existing_items()
        
        with open(csv_path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            
            print("Pre-processing CSV data...")
            
            # New: Map to store parent field categories (ID -> Category Name)
            id_category_map = {} 
            
            for row in rows:
                # 1. Build ID Map First (Single pass requires sorted order, but we'll be robust)
                raw_type = row.get('type', 'General')
                row_id = row.get('id')
                parent_id = row.get('parent_field')
                
                # Logic to determine Category
                category = "General"
                
                if raw_type == 'option':
                    # SEPARATE CATEGORY FOR OPTIONS
                    category = "Tagger_Options"
                else:
                    # It's a parent field (or orphan), determine category from own type
                    category = self._sanitize_name(raw_type).capitalize()
                    if row_id:
                        id_category_map[row_id] = category
                
                # Store category in row for analyze_changes to use
                row['agent_category'] = category

                # 2. Generate Sanitized Name Base (without category)
                raw_name = row['dc_name']
                row['sanitized_name'] = self._sanitize_name(raw_name)

                # 3. Normalize ALL content columns first to prevent ghost updates
                # We iterate through both maps to ensure all columns are clean
                all_cols = set(AGENT_LOCALE_MAP.keys()) | set(PORTAL_LOCALE_MAP.keys())
                for col in all_cols:
                    if col in row:
                        row[col] = self._normalize_content(row.get(col, ''))

        plan = self.analyze_changes(rows, do_agent, do_portal, aggressive_norm)
        self.print_preview(plan, rows)
        
        if not plan['create'] and not plan['update']:
            print("No changes needed. Exiting.")
            return

        if self.get_user_confirmation("\nDo you want to proceed with these changes?"):
            self.execute_plan(plan)
            print("\nDone!")
        else:
            print("\nOperation cancelled by user.")

if __name__ == '__main__':
    csv_file = sys.argv[1] if len(sys.argv) > 1 else INPUT_CSV_FILE
    
    if csv_file == 'path/to/your_file.csv':
        print("Please configure the INPUT_CSV_FILE variable or pass the file path as an argument.")
    else:
        importer = ZendeskDCImporter()
        importer.process(csv_file)